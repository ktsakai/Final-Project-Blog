[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nFinal Project Related Assignment\n\n\n\n\n\nDraft of Final Project\n\n\n\n\n\n\nJul 30, 2023\n\n\nKaelyn Sakai\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nFirst File\n\n\n\n\n\nTesting Quarto\n\n\n\n\n\n\nJul 26, 2023\n\n\nKaelyn Sakai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Topics.html",
    "href": "posts/Topics.html",
    "title": "First File",
    "section": "",
    "text": "We are going to look at data from the 20 Newsgroups dataset. These are postings to newsgroups in 20 different categories.\nScikit-learn has a function for downloading the data. See: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n\n\nLatent Dirichlet Allocation: a topic model that generates topics based on a set of documents’ word frequencies.\n\nGet a “dictionary” that has IDs for all the words along with a record of their word frequencies.\nUse our “bag of words” to generate a list for each document containing its words and their frequencies\nUse gensim to generate an LDA model\n\n\n\n\n\n“Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.”\ngensim website\n\n\nfrom sklearn.datasets import fetch_20newsgroups\n\n\ndata = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"))\n\n\nprint(data.DESCR)\n\n\nx = data.data\n\n\nlen(x)\n\n\nx[0]\n\n\ndata.target_names\n\n\ndata.target\n\nWe use NLTK to pre-process the words.\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n\nmyStopWords = list(punctuation) + stopwords.words('english')\n\n\nx[0]\n\n\n[w for w in word_tokenize(x[0].lower()) if w not in myStopWords]\n\n\ndocs = []\nfor i in x:\n    docs.append([w for w in word_tokenize(i.lower()) if w not in myStopWords])\n\n\ndocs[0]\n\n\nfrom nltk.stem.porter import PorterStemmer\n#from nltk.stem import LancasterStemmer\n\n\n# Create p_stemmer of class PorterStemmer\np_stemmer = PorterStemmer()\n\n\ndocs_stemmed = []\nfor i in docs:\n    docs_stemmed.append([p_stemmer.stem(w) for w in i])\n\n\ndocs_stemmed[0]\n\nHere we use gensim to make the dictionary and corpus structures, and to employ the LDA model to extract groups (aka topics) and the distribution of words for each topic.\n\nfrom gensim import corpora, models\nimport gensim\n\n\ndictionary = corpora.Dictionary(docs_stemmed)\n\n\nlen(dictionary)\n\n\ndictionary.filter_extremes(no_below=10, no_above=0.5)\n# could also trim with keep_n=1000 or similar to keep only the top words\n\n\nlen(dictionary)\n\n\nprint(dictionary.token2id)\n\n\nprint(dictionary.token2id['patient'])\n\n\ndictionary[1668]\n\n\ncorpus = [dictionary.doc2bow(text) for text in docs_stemmed]\n\n\nprint(corpus[30])\n\n\ndictionary[276]\n\n\ndocs_stemmed[30]\n\n\nwordid = corpus[30][0]\nprint(dictionary[wordid[0]],wordid[1])\n\n\nfor i in corpus[30]:\n    print(dictionary[i[0]], i[1])\n\n\nldamodel = gensim.models.ldamodel.LdaModel(corpus, \n                                           num_topics=20, \n                                           id2word = dictionary, \n                                           passes=5)\n\n\nldamodel.show_topics(num_topics=20)\n\n\nfor i in ldamodel.print_topics(num_topics=20, num_words=20):\n    print(i[0])\n    print(i[1])\n    print('\\n')\n\n\ndata.target_names\n\n\nimport matplotlib.pyplot as plt\nimport re\n\n\nre.split(re.escape(' + ') + '|' + re.escape('*'), 'hi + me*4')\n\n\nfig,ax = plt.subplots(5,4,figsize=(15,20))\nax = ax.flatten()\nfor i in ldamodel.print_topics(num_topics=20, num_words=20):\n    x = []\n    y = []\n    count = 0\n    for j in re.split(re.escape(' + ') + '|' + re.escape('*'), i[1]):\n        if count % 2 == 0:\n            y.insert(0,float(j))\n        else:\n            x.insert(0,j)\n        count += 1\n    ax[i[0]].barh(x,y,height=0.5)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Topics.html#lda",
    "href": "posts/Topics.html#lda",
    "title": "First File",
    "section": "",
    "text": "Latent Dirichlet Allocation: a topic model that generates topics based on a set of documents’ word frequencies.\n\nGet a “dictionary” that has IDs for all the words along with a record of their word frequencies.\nUse our “bag of words” to generate a list for each document containing its words and their frequencies\nUse gensim to generate an LDA model"
  },
  {
    "objectID": "posts/Topics.html#gensim",
    "href": "posts/Topics.html#gensim",
    "title": "First File",
    "section": "",
    "text": "“Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.”\ngensim website\n\n\nfrom sklearn.datasets import fetch_20newsgroups\n\n\ndata = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"))\n\n\nprint(data.DESCR)\n\n\nx = data.data\n\n\nlen(x)\n\n\nx[0]\n\n\ndata.target_names\n\n\ndata.target\n\nWe use NLTK to pre-process the words.\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n\nmyStopWords = list(punctuation) + stopwords.words('english')\n\n\nx[0]\n\n\n[w for w in word_tokenize(x[0].lower()) if w not in myStopWords]\n\n\ndocs = []\nfor i in x:\n    docs.append([w for w in word_tokenize(i.lower()) if w not in myStopWords])\n\n\ndocs[0]\n\n\nfrom nltk.stem.porter import PorterStemmer\n#from nltk.stem import LancasterStemmer\n\n\n# Create p_stemmer of class PorterStemmer\np_stemmer = PorterStemmer()\n\n\ndocs_stemmed = []\nfor i in docs:\n    docs_stemmed.append([p_stemmer.stem(w) for w in i])\n\n\ndocs_stemmed[0]\n\nHere we use gensim to make the dictionary and corpus structures, and to employ the LDA model to extract groups (aka topics) and the distribution of words for each topic.\n\nfrom gensim import corpora, models\nimport gensim\n\n\ndictionary = corpora.Dictionary(docs_stemmed)\n\n\nlen(dictionary)\n\n\ndictionary.filter_extremes(no_below=10, no_above=0.5)\n# could also trim with keep_n=1000 or similar to keep only the top words\n\n\nlen(dictionary)\n\n\nprint(dictionary.token2id)\n\n\nprint(dictionary.token2id['patient'])\n\n\ndictionary[1668]\n\n\ncorpus = [dictionary.doc2bow(text) for text in docs_stemmed]\n\n\nprint(corpus[30])\n\n\ndictionary[276]\n\n\ndocs_stemmed[30]\n\n\nwordid = corpus[30][0]\nprint(dictionary[wordid[0]],wordid[1])\n\n\nfor i in corpus[30]:\n    print(dictionary[i[0]], i[1])\n\n\nldamodel = gensim.models.ldamodel.LdaModel(corpus, \n                                           num_topics=20, \n                                           id2word = dictionary, \n                                           passes=5)\n\n\nldamodel.show_topics(num_topics=20)\n\n\nfor i in ldamodel.print_topics(num_topics=20, num_words=20):\n    print(i[0])\n    print(i[1])\n    print('\\n')\n\n\ndata.target_names\n\n\nimport matplotlib.pyplot as plt\nimport re\n\n\nre.split(re.escape(' + ') + '|' + re.escape('*'), 'hi + me*4')\n\n\nfig,ax = plt.subplots(5,4,figsize=(15,20))\nax = ax.flatten()\nfor i in ldamodel.print_topics(num_topics=20, num_words=20):\n    x = []\n    y = []\n    count = 0\n    for j in re.split(re.escape(' + ') + '|' + re.escape('*'), i[1]):\n        if count % 2 == 0:\n            y.insert(0,float(j))\n        else:\n            x.insert(0,j)\n        count += 1\n    ax[i[0]].barh(x,y,height=0.5)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Final Project Notebook Draft.html",
    "href": "posts/Final Project Notebook Draft.html",
    "title": "Final Project Related Assignment",
    "section": "",
    "text": "Kaelyn Sakai • July 2023"
  },
  {
    "objectID": "posts/Final Project Notebook Draft.html#project-introduction",
    "href": "posts/Final Project Notebook Draft.html#project-introduction",
    "title": "Final Project Related Assignment",
    "section": "Project Introduction",
    "text": "Project Introduction\nThe dataset I that I plan on investigating further is the Spotify Top Songs Data from Kaggle. The data contains information about a variety of Billboard top songs. I wanted to analyze this, because I have always been interested in music trends and music in general. Music throughout history is consistently changing, and exploring this dataset can help to further understand the types of songs that have been popular recently. The main research question I will be addressing is, “What types of songs were the most popular in the 2010’s and can correlations be drawn between the different variables that contribute to the song popularity?”."
  },
  {
    "objectID": "posts/Final Project Notebook Draft.html#data-exploration-and-introduction",
    "href": "posts/Final Project Notebook Draft.html#data-exploration-and-introduction",
    "title": "Final Project Related Assignment",
    "section": "Data Exploration and Introduction",
    "text": "Data Exploration and Introduction\nThis data contains some of the top songs between the years 2010 through 2019. This is a brief overview of the data and what each variable represents.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntop genre\nGenre of the track\n\n\nyear\nThe year the song appeared on Billboard\n\n\nbpm\nBeats per minute of the song\n\n\nnrgy\nEnergy: The energy rating of the song - the higher the nrgy score, the more energetic the song is\n\n\ndnce\nDance: The danceability rating of the song - the higher the dnce score, the easier it is to dance to\n\n\ndB\nDecibels: The overall loudness - the higher the dB, the louder the song\n\n\nlive\nThe higher the value, the more likely the song is a live recording\n\n\nval\nValence: The mood of the song - the higher the valence, the more cheerful the song is\n\n\ndur\nDuration: Length of the song in seconds\n\n\nacous\nAcoustics: The higher the acoustic value, the more acoustic the song is\n\n\nspch\nSpeechiness: The higher the value, the song contains more spoken words\n\n\npop\nPopularity: Measures how popular the song was\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\ndf = pd.read_csv('spotify.csv',encoding='latin-1')\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\ntitle\nartist\ntop genre\nyear\nbpm\nnrgy\ndnce\ndB\nlive\nval\ndur\nacous\nspch\npop\n\n\n\n\n0\n1\nHey, Soul Sister\nTrain\nneo mellow\n2010\n97\n89\n67\n-4\n8\n80\n217\n19\n4\n83\n\n\n1\n2\nLove The Way You Lie\nEminem\ndetroit hip hop\n2010\n87\n93\n75\n-5\n52\n64\n263\n24\n23\n82\n\n\n2\n3\nTiK ToK\nKesha\ndance pop\n2010\n120\n84\n76\n-3\n29\n71\n200\n10\n14\n80\n\n\n3\n4\nBad Romance\nLady Gaga\ndance pop\n2010\n119\n92\n70\n-4\n8\n71\n295\n0\n4\n79\n\n\n4\n5\nJust the Way You Are\nBruno Mars\npop\n2010\n109\n84\n64\n-5\n9\n43\n221\n2\n4\n78\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n598\n599\nFind U Again (feat. Camila Cabello)\nMark Ronson\ndance pop\n2019\n104\n66\n61\n-7\n20\n16\n176\n1\n3\n75\n\n\n599\n600\nCross Me (feat. Chance the Rapper & PnB Rock)\nEd Sheeran\npop\n2019\n95\n79\n75\n-6\n7\n61\n206\n21\n12\n75\n\n\n600\n601\nNo Brainer (feat. Justin Bieber, Chance the Ra...\nDJ Khaled\ndance pop\n2019\n136\n76\n53\n-5\n9\n65\n260\n7\n34\n70\n\n\n601\n602\nNothing Breaks Like a Heart (feat. Miley Cyrus)\nMark Ronson\ndance pop\n2019\n114\n79\n60\n-6\n42\n24\n217\n1\n7\n69\n\n\n602\n603\nKills You Slowly\nThe Chainsmokers\nelectropop\n2019\n150\n44\n70\n-9\n13\n23\n213\n6\n6\n67\n\n\n\n\n603 rows × 15 columns\n\n\n\nThe table above shows an overview of the dataset I will be using. There are 603 songs included, with 14 columns describing them.\n\ndf.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nbpm\nnrgy\ndnce\ndB\nlive\nval\ndur\nacous\nspch\npop\n\n\n\n\ncount\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n\n\nmean\n302.000000\n2014.592040\n118.545605\n70.504146\n64.379768\n-5.578773\n17.774461\n52.225539\n224.674959\n14.326700\n8.358209\n66.520730\n\n\nstd\n174.215384\n2.607057\n24.795358\n16.310664\n13.378718\n2.798020\n13.102543\n22.513020\n34.130059\n20.766165\n7.483162\n14.517746\n\n\nmin\n1.000000\n2010.000000\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n0.000000\n134.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n151.500000\n2013.000000\n100.000000\n61.000000\n57.000000\n-6.000000\n9.000000\n35.000000\n202.000000\n2.000000\n4.000000\n60.000000\n\n\n50%\n302.000000\n2015.000000\n120.000000\n74.000000\n66.000000\n-5.000000\n12.000000\n52.000000\n221.000000\n6.000000\n5.000000\n69.000000\n\n\n75%\n452.500000\n2017.000000\n129.000000\n82.000000\n73.000000\n-4.000000\n24.000000\n69.000000\n239.500000\n17.000000\n9.000000\n76.000000\n\n\nmax\n603.000000\n2019.000000\n206.000000\n98.000000\n97.000000\n-2.000000\n74.000000\n98.000000\n424.000000\n99.000000\n48.000000\n99.000000\n\n\n\n\n\n\n\n\ndf.plot(kind='scatter',x='year',y='val',color='lightblue')\n\n&lt;Axes: xlabel='year', ylabel='val'&gt;\n\n\n\n\n\n\nc = df['artist'].value_counts().nlargest(15)\nc.plot(kind='bar',color='pink')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.plot(kind='scatter',x='nrgy',y='pop',color='purple')\n\n&lt;Axes: xlabel='nrgy', ylabel='pop'&gt;\n\n\n\n\n\n\ng = df['top genre'].value_counts().nlargest(15)\ng.plot(kind='bar',color='mediumturquoise')\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/Final Project Notebook Draft.html#data-analysis",
    "href": "posts/Final Project Notebook Draft.html#data-analysis",
    "title": "Final Project Related Assignment",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nimport requests\nimport matplotlib.pyplot as plt\nimport nltk\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nnltk.download('opinion_lexicon')\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nsia = vader.SentimentIntensityAnalyzer()\n\n\ncol_list = df[\"title\"].values.tolist()\ntitle = ' '.join(col_list)\ntitle_lower = title.lower()\n\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\ndef tokenize(string):\n  sent = sent_tokenize(title_lower)\n  words = []\n  for s in sent:\n      for w in word_tokenize(s):\n          words.append(w)\n  return words\n\nwords = tokenize(title_lower)\nprint(words)\n\n['hey', ',', 'soul', 'sister', 'love', 'the', 'way', 'you', 'lie', 'tik', 'tok', 'bad', 'romance', 'just', 'the', 'way', 'you', 'are', 'baby', 'dynamite', 'secrets', 'empire', 'state', 'of', 'mind', '(', 'part', 'ii', ')', 'broken', 'down', 'only', 'girl', '(', 'in', 'the', 'world', ')', 'club', 'ca', \"n't\", 'handle', 'me', '(', 'feat', '.', 'david', 'guetta', ')', 'marry', 'you', 'cooler', 'than', 'me', '-', 'single', 'mix', 'telephone', 'like', 'a', 'g6', 'omg', '(', 'feat', '.', 'will.i.am', ')', 'eenie', 'meenie', 'the', 'time', '(', 'dirty', 'bit', ')', 'alejandro', 'your', 'love', 'is', 'my', 'drug', 'meet', 'me', 'halfway', 'whataya', 'want', 'from', 'me', 'take', 'it', 'off', 'misery', 'all', 'the', 'right', 'moves', 'animal', 'naturally', 'i', 'like', 'it', 'teenage', 'dream', 'california', 'gurls', '3', 'my', 'first', 'kiss', '-', 'feat', '.', 'ke', '$', 'ha', 'blah', 'blah', 'blah', '(', 'feat', '.', '3oh', '!', '3', ')', 'imma', 'be', 'try', 'sleeping', 'with', 'a', 'broken', 'heart', 'sexy', 'bitch', '(', 'feat', '.', 'akon', ')', 'bound', 'to', 'you', '-', 'burlesque', 'original', 'motion', 'picture', 'soundtrack', 'if', 'i', 'had', 'you', 'rock', 'that', 'body', 'dog', 'days', 'are', 'over', 'something', \"'s\", 'got', 'a', 'hold', 'on', 'me', '-', 'burlesque', 'original', 'motion', 'picture', 'soundtrack', 'does', \"n't\", 'mean', 'anything', 'hard', 'loca', 'you', 'lost', 'me', 'not', 'myself', 'tonight', 'written', 'in', 'the', 'stars', '(', 'feat', '.', 'eric', 'turner', ')', 'dj', 'got', 'us', 'fallin', \"'\", 'in', 'love', '(', 'feat', '.', 'pitbull', ')', 'castle', 'walls', '(', 'feat', '.', 'christina', 'aguilera', ')', 'break', 'your', 'heart', 'hello', 'a', 'thousand', 'years', 'someone', 'like', 'you', 'give', 'me', 'everything', 'just', 'the', 'way', 'you', 'are', 'rolling', 'in', 'the', 'deep', 'run', 'the', 'world', '(', 'girls', ')', 'moves', 'like', 'jagger', '-', 'studio', 'recording', 'from', 'the', 'voice', 'performance', 'love', 'on', 'top', 'grenade', 'tonight', 'tonight', 'what', 'the', 'hell', 'born', 'this', 'way', 'monster', 'marry', 'you', 'best', 'thing', 'i', 'never', 'had', 'party', 'rock', 'anthem', 'we', 'r', 'who', 'we', 'r', 'price', 'tag', 'good', 'life', 'just', 'can\\x92t', 'get', 'enough', 'on', 'the', 'floor', 'what', \"'s\", 'my', 'name', '?', 'yeah', '3x', 'without', 'you', '(', 'feat', '.', 'usher', ')', 'sexy', 'and', 'i', 'know', 'it', 'the', 'edge', 'of', 'glory', 'e.t', '.', 'till', 'the', 'world', 'ends', 'i', 'wan', 'na', 'go', 'blow', 'you', 'and', 'i', 'judas', 'tonight', '(', 'i', \"'m\", 'fuckin', \"'\", 'you', ')', 'please', 'do', \"n't\", 'go', 'we', 'found', 'love', 'marry', 'the', 'night', '1+1', 'hold', 'it', 'against', 'me', 'i', \"'m\", 'into', 'you', 'papi', 'cheers', '(', 'drink', 'to', 'that', ')', 's', '&', 'm', 'remix', 'written', 'in', 'the', 'stars', '(', 'feat', '.', 'eric', 'turner', ')', 'jar', 'of', 'hearts', 'castle', 'walls', '(', 'feat', '.', 'christina', 'aguilera', ')', 'turning', 'page', 'super', 'bass', 'raise', 'your', 'glass', 'invading', 'my', 'mind', 'moment', '4', 'life', '-', 'album', 'version', '(', 'edited', ')', 'last', 'friday', 'night', '(', 't.g.i.f', '.', ')', 'firework', 'muny', '-', 'album', 'version', '(', 'edited', ')', 'titanium', '(', 'feat', '.', 'sia', ')', 'locked', 'out', 'of', 'heaven', 'paradise', 'payphone', 'what', 'makes', 'you', 'beautiful', 'i', 'knew', 'you', 'were', 'trouble', '.', 'call', 'me', 'maybe', 'love', 'you', 'like', 'a', 'love', 'song', 'set', 'fire', 'to', 'the', 'rain', 'we', 'are', 'never', 'ever', 'getting', 'back', 'together', 'stronger', '(', 'what', 'does', \"n't\", 'kill', 'you', ')', 'try', 'starships', 'one', 'more', 'night', 'good', 'time', 'glad', 'you', 'came', 'beauty', 'and', 'a', 'beat', 'international', 'love', 'some', 'nights', 'boyfriend', 'part', 'of', 'me', 'domino', 'where', 'have', 'you', 'been', 'wide', 'awake', 'the', 'one', 'that', 'got', 'away', 'dance', 'again', 'turn', 'up', 'the', 'music', 'lights', '-', 'single', 'version', 'we', 'are', 'young', '(', 'feat', '.', 'janelle', 'monáe', ')', 'diamonds', 'do', \"n't\", 'stop', 'the', 'party', '(', 'feat', '.', 'tjr', ')', 'you', 'da', 'one', 'stereo', 'hearts', '(', 'feat', '.', 'adam', 'levine', ')', 'it', 'will', 'rain', 'blow', 'me', '(', 'one', 'last', 'kiss', ')', 'underneath', 'the', 'tree', 'wake', 'me', 'up', 'story', 'of', 'my', 'life', 'just', 'give', 'me', 'a', 'reason', '(', 'feat', '.', 'nate', 'ruess', ')', 'hall', 'of', 'fame', 'roar', 'we', 'ca', \"n't\", 'stop', 'do', \"n't\", 'you', 'worry', 'child', '-', 'radio', 'edit', 'get', 'lucky', '(', 'feat', '.', 'pharrell', 'williams', '&', 'nile', 'rodgers', ')', '-', 'radio', 'edit', 'wrecking', 'ball', 'impossible', 'blurred', 'lines', 'heart', 'attack', 'we', 'are', 'never', 'ever', 'getting', 'back', 'together', 'die', 'young', 'clarity', 'summertime', 'sadness', '(', 'lana', 'del', 'rey', 'vs.', 'cedric', 'gervais', ')', '-', 'cedric', 'gervais', 'remix', 'under', 'control', 'everybody', 'talks', 'hold', 'on', ',', 'we', \"'re\", 'going', 'home', 'best', 'song', 'ever', 'kiss', 'you', 'sweet', 'nothing', '(', 'feat', '.', 'florence', 'welch', ')', 'lose', 'yourself', 'to', 'dance', 'work', 'bitch', 'brave', 'ca', \"n't\", 'hold', 'us', '(', 'feat', '.', 'ray', 'dalton', ')', 'feel', 'this', 'moment', '(', 'feat', '.', 'christina', 'aguilera', ')', 'beneath', 'your', 'beautiful', 'let', 'me', 'love', 'you', '(', 'until', 'you', 'learn', 'to', 'love', 'yourself', ')', 'thrift', 'shop', '(', 'feat', '.', 'wanz', ')', 'if', 'i', 'lose', 'myself', '-', 'alesso', 'vs', 'onerepublic', 'the', 'way', 'suit', '&', 'tie', '#', 'thatpower', 'i', 'love', 'it', '(', 'feat', '.', 'charli', 'xcx', ')', 'play', 'hard', '(', 'feat', '.', 'ne-yo', '&', 'akon', ')', '-', 'new', 'edit', 'daylight', 'love', 'somebody', 'a', 'little', 'party', 'never', 'killed', 'nobody', '(', 'all', 'we', 'got', ')', 'move', 'walks', 'like', 'rihanna', 'rock', 'n', 'roll', 'heartbreaker', 'mirrors', '-', 'radio', 'edit', 'next', 'to', 'me', 'made', 'in', 'the', 'usa', 'clown', 'girl', 'on', 'fire', '(', 'feat', '.', 'nicki', 'minaj', ')', '-', 'inferno', 'version', 'tko', 'come', '&', 'get', 'it', 'live', 'it', 'up', 'we', 'own', 'the', 'night', 'atlas', '-', 'from', '\\x93the', 'hunger', 'games', ':', 'catching', 'fire\\x94', 'soundtrack', 'what', 'about', 'love', 'take', 'back', 'the', 'night', 'applause', 'anything', 'could', 'happen', 'finally', 'found', 'you', 'pom', 'poms', '#', 'beautiful', 'how', 'ya', 'doin', \"'\", '?', '(', 'feat', '.', 'missy', 'elliott', ')', 'crazy', 'kids', '(', 'feat', '.', 'will.i.am', ')', 'ooh', 'la', 'la', '(', 'from', '``', 'the', 'smurfs', '2', \"''\", ')', 'people', 'like', 'us', 'overdose', 'right', 'now', '-', 'dyro', 'radio', 'edit', 'give', 'it', '2', 'u', 'foolish', 'games', 'outta', 'nowhere', '(', 'feat', '.', 'danny', 'mercer', ')', 'freak', 'all', 'of', 'me', 'stay', 'with', 'me', 'summer', 'happy', '-', 'from', '``', 'despicable', 'me', '2', \"''\", 'rude', 'shake', 'it', 'off', 'dark', 'horse', 'hey', 'brother', 'maps', 'treasure', 'let', 'her', 'go', 'problem', 'pompeii', 'team', 'love', 'me', 'again', 'latch', 'adore', 'you', 'love', 'never', 'felt', 'so', 'good', 'burn', 'she', 'looks', 'so', 'perfect', 'fancy', 'talk', 'dirty', '(', 'feat', '.', '2', 'chainz', ')', 'gorilla', 'human', 'young', 'girls', 'wiggle', '(', 'feat', '.', 'snoop', 'dogg', ')', 'love', 'runs', 'out', 'this', 'is', 'how', 'we', 'do', 'mmm', 'yeah', '(', 'feat', '.', 'pitbull', ')', 'a', 'little', 'party', 'never', 'killed', 'nobody', '(', 'all', 'we', 'got', ')', '#', 'selfie', 'partition', 'birthday', 'g.u.y', '.', 'stay', 'the', 'night', '-', 'featuring', 'hayley', 'williams', 'of', 'paramore', 'let', 'it', 'go', '-', 'from', '``', 'frozen', '/', 'single', 'version', 'wings', 'ca', \"n't\", 'remember', 'to', 'forget', 'you', '(', 'feat', '.', 'rihanna', ')', 'shot', 'me', 'down', '(', 'feat', '.', 'skylar', 'grey', ')', '-', 'radio', 'edit', 'say', 'something', 'a', 'sky', 'full', 'of', 'stars', 'come', 'get', 'it', 'bae', 'chandelier', 'xo', 'we', 'are', 'one', '(', 'ole', 'ola', ')', '[', 'the', 'official', '2014', 'fifa', 'world', 'cup', 'song', ']', 'not', 'about', 'angels', 'drunk', 'in', 'love', 'anaconda', 'boom', 'clap', '-', 'from', 'the', 'motion', 'picture', 'das', 'schicksal', 'ist', 'ein', 'mieser', 'verräter', 'la', 'la', 'la', '(', 'brasil', '2014', ')', '(', 'feat', '.', 'carlinhos', 'brown', ')', 'tee', 'shirt', '-', 'soundtrack', 'version', 'words', 'as', 'weapons', 'you', \"'re\", 'mine', '(', 'eternal', ')', 'sheezus', 'cannonball', 'it', \"'s\", 'on', 'again', '-', 'main', 'soundtrack', 'i', 'luh', 'ya', 'papi', 'not', 'a', 'bad', 'thing', 'thinking', 'out', 'loud', 'i', \"'m\", 'not', 'the', 'only', 'one', 'the', 'hills', 'love', 'yourself', 'uptown', 'funk', 'take', 'me', 'to', 'church', 'sugar', 'sorry', 'fourfiveseconds', 'love', 'me', 'like', 'you', 'do', '-', 'from', '``', 'fifty', 'shades', 'of', 'grey', \"''\", 'earned', 'it', '(', 'fifty', 'shades', 'of', 'grey', ')', '-', 'from', 'the', '``', 'fifty', 'shades', 'of', 'grey', \"''\", 'soundtrack', 'what', 'do', 'you', 'mean', '?', 'stitches', 'want', 'to', 'want', 'me', 'my', 'house', 'waves', '-', 'robin', 'schulz', 'radio', 'edit', 'night', 'changes', 'how', 'deep', 'is', 'your', 'love', 'never', 'forget', 'you', 'love', 'me', 'harder', 'animals', 'blame', 'worth', 'it', 'break', 'free', 'do', \"n't\", 'elastic', 'heart', 'rather', 'be', '(', 'feat', '.', 'jess', 'glynne', ')', 'hello', 'dear', 'future', 'husband', '43776', 'the', 'heart', 'wants', 'what', 'it', 'wants', 'hey', 'mama', '(', 'feat', '.', 'nicki', 'minaj', ',', 'bebe', 'rexha', '&', 'afrojack', ')', 'genie', 'in', 'a', 'bottle', 'company', 'sing', 'jealous', '-', 'remix', 'really', 'do', \"n't\", 'care', 'downtown', '(', 'feat', '.', 'melle', 'mel', ',', 'grandmaster', 'caz', ',', 'kool', 'moe', 'dee', '&', 'eric', 'nally', ')', 'only', 'love', 'can', 'hurt', 'like', 'this', 'heartbeat', 'song', 'up', 'trumpets', 'runnin', \"'\", '(', 'lose', 'it', 'all', ')', 'same', 'old', 'love', 'i', 'want', 'you', 'to', 'know', 'lips', 'are', 'movin', 'i', \"'ll\", 'show', 'you', 'here', 'i', 'lived', 'fireball', '(', 'feat', '.', 'john', 'ryan', ')', 'easy', 'love', 'the', 'feeling', 'i', 'really', 'like', 'you', 'bo', '$', '$', 'sugar', 'focus', 'all', 'about', 'that', 'bass', 'on', 'my', 'mind', 'love', 'me', 'like', 'you', 'broken', 'arrows', 'booty', 'what', 'do', 'you', 'mean', '?', '-', 'acoustic', 'mark', 'my', 'words', 'lay', 'it', 'all', 'on', 'me', 'american', 'oxygen', 'bang', 'bang', 'reality', '-', 'radio', 'edit', 'alive', 'sugar', '(', 'feat', '.', 'francesco', 'yates', ')', 'been', 'you', 'prayer', 'in', 'c', '-', 'robin', 'schulz', 'radio', 'edit', 'see', 'you', 'again', '(', 'feat', '.', 'charlie', 'puth', ')', 'heroes', '(', 'we', 'could', 'be', ')', 'feel', 'the', 'light', '-', 'from', 'the', '``', 'home', \"''\", 'soundtrack', 'perfect', 'ghosttown', 'bang', 'my', 'head', '(', 'feat', '.', 'sia', '&', 'fetty', 'wap', ')', 'bloodstream', 'living', 'for', 'love', 'baby', 'do', \"n't\", 'lie', 'do', \"n't\", 'be', 'so', 'hard', 'on', 'yourself', 'steal', 'my', 'girl', 'celebrate', '(', 'from', 'the', 'original', 'motion', 'picture', '``', 'penguins', 'of', 'madagascar', \"''\", ')', 'we', 'are', 'here', 'st', 'jude', 'yesterday', '(', 'feat', '.', 'bebe', 'rexha', ')', 'time', 'of', 'our', 'lives', 'sparks', 'mr.', 'put', 'it', 'down', 'legendary', 'lovers', 'spark', 'the', 'fire', 'run', 'run', 'run', 'let', 'me', 'be', 'your', 'lover', 'dangerous', 'l.a.love', '(', 'la', 'la', ')', 'the', 'hills', 'love', 'yourself', 'cake', 'by', 'the', 'ocean', 'do', \"n't\", 'let', 'me', 'down', 'in', 'the', 'name', 'of', 'love', 'into', 'you', 'this', 'is', 'what', 'you', 'came', 'for', 'million', 'reasons', 'needed', 'me', '7', 'years', 'ca', \"n't\", 'stop', 'the', 'feeling', '!', '(', 'original', 'song', 'from', 'dreamworks', 'animation', \"'s\", '``', 'trolls', \"''\", ')', 'work', 'from', 'home', '(', 'feat', '.', 'ty', 'dolla', '$', 'ign', ')', 'scars', 'to', 'your', 'beautiful', 'like', 'i', \"'m\", 'gon', 'na', 'lose', 'you', '(', 'feat', '.', 'john', 'legend', ')', 'work', 'stitches', 'me', ',', 'myself', '&', 'i', 'i', 'took', 'a', 'pill', 'in', 'ibiza', '-', 'seeb', 'remix', 'dangerous', 'woman', 'starving', 'shout', 'out', 'to', 'my', 'ex', 'electric', 'love', 'confident', 'too', 'good', 'roses', 'cold', 'water', '(', 'feat', '.', 'justin', 'bieber', '&', 'mø', ')', 'me', 'too', 'light', 'it', 'up', '(', 'feat', '.', 'nyla', '&', 'fuse', 'odg', ')', '[', 'remix', ']', 'ai', \"n't\", 'your', 'mama', 'close', 'toothbrush', 'all', 'we', 'know', 'final', 'song', 'company', 'hands', 'to', 'myself', 'all', 'i', 'ask', 'just', 'like', 'fire', '(', 'from', 'the', 'original', 'motion', 'picture', '``', 'alice', 'through', 'the', 'looking', 'glass', \"''\", ')', 'no', 'kill', 'em', 'with', 'kindness', 'cool', 'girl', 'runnin', \"'\", '(', 'lose', 'it', 'all', ')', 'here', 'perfect', 'illusion', 'pillowtalk', 'out', 'of', 'the', 'woods', 'rise', 'wherever', 'i', 'go', 'body', 'say', 'do', \"n't\", 'be', 'a', 'fool', 'like', 'i', 'would', 'cheap', 'thrills', 'i', 'got', 'you', 'run', 'away', 'with', 'me', 'cruel', '(', 'feat', '.', 'zayn', ')', 'send', 'my', 'love', '(', 'to', 'your', 'new', 'lover', ')', 'wtf', '(', 'where', 'they', 'from', ')', 'desire', 'when', 'we', 'were', 'young', 'i', 'know', 'what', 'you', 'did', 'last', 'summer', 'wish', 'that', 'you', 'were', 'here', '-', 'from', '\\x93miss', 'peregrine\\x92s', 'home', 'for', 'peculiar', 'children\\x94', 'original', 'motion', 'picture', 'hurts', 'change', 'make', 'me', '...', '(', 'feat', '.', 'g-eazy', ')', 'keeping', 'your', 'head', 'up', 'true', 'colors', 'make', 'me', 'like', 'you', 'champagne', 'problems', 'blown', 'start', 'pep', 'rally', 'higher', 'invitation', 'one', 'call', 'away', '(', 'feat', '.', 'tyga', ')', '-', 'remix', 'beautiful', 'birds', '(', 'feat', '.', 'birdy', ')', 'little', 'lies', 'do', 'you', 'wan', 'na', 'come', 'over', '?', 'burnitup', '!', 'picky', '-', 'remix', 'behind', 'your', 'back', 'million', 'years', 'ago', 'shape', 'of', 'you', 'closer', 'starboy', 'treat', 'you', 'better', 'that', \"'s\", 'what', 'i', 'like', 'let', 'me', 'love', 'you', 'i', 'feel', 'it', 'coming', 'mercy', 'side', 'to', 'side', 'stay', 'it', 'ai', \"n't\", 'me', '(', 'with', 'selena', 'gomez', ')', 'malibu', 'something', 'just', 'like', 'this', 'rockabye', '(', 'feat', '.', 'sean', 'paul', '&', 'anne-marie', ')', 'i', 'don\\x92t', 'wan', 'na', 'live', 'forever', '(', 'fifty', 'shades', 'darker', ')', 'my', 'way', 'i', \"'m\", 'the', 'one', '(', 'feat', '.', 'justin', 'bieber', ',', 'quavo', ',', 'chance', 'the', 'rapper', '&', 'lil', 'wayne', ')', 'praying', 'despacito', '-', 'remix', 'the', 'greatest', 'there', 'for', 'you', 'paris', 'crying', 'in', 'the', 'club', 'mama', 'slide', '(', 'feat', '.', 'frank', 'ocean', '&', 'migos', ')', 'swish', 'swish', 'chained', 'to', 'the', 'rhythm', 'cold', '(', 'feat', '.', 'future', ')', 'love', 'reggaetón', 'lento', '(', 'remix', ')', 'all', 'i', 'ask', 'first', 'time', 'the', 'cure', 'how', 'far', 'i', \"'ll\", 'go', '-', 'from', '``', 'moana', \"''\", 'bodak', 'yellow', 'rich', 'love', '(', 'with', 'seeb', ')', 'tired', 'came', 'here', 'for', 'love', '24k', 'magic', 'strip', 'that', 'down', '(', 'feat', '.', 'quavo', ')', 'cut', 'to', 'the', 'feeling', 'ok', '-', 'spotify', 'version', 'bon', 'appétit', 'summer', 'bummer', '(', 'feat', '.', 'a', '$', 'ap', 'rocky', '&', 'playboi', 'carti', ')', 'get', 'low', '(', 'with', 'liam', 'payne', ')', 'kissing', 'strangers', 'slow', 'hands', 'younger', 'now', 'body', 'moves', 'reality', '(', 'feat', '.', 'janieck', 'devy', ')', '-', 'radio', 'edit', 'angel', 'touch', '(', 'feat', '.', 'kid', 'ink', ')', 'we', 'do', \"n't\", 'talk', 'anymore', '-', 'droeloe', 'remix', 'love', 'incredible', '(', 'feat', '.', 'camila', 'cabello', ')', 'no', 'vacancy', '(', 'with', 'sebastián', 'yatra', ')', 'rich', 'boy', 'lust', 'for', 'life', '(', 'with', 'the', 'weeknd', ')', 'greenlight', '(', 'feat', '.', 'flo', 'rida', '&', 'lunchmoney', 'lewis', ')', 'influence', 'remember', 'i', 'told', 'you', 'messin', \"'\", 'around', 'water', 'under', 'the', 'bridge', 'free', 'me', 'kissing', 'strangers', '-', 'remix', 'a', 'l', 'i', 'e', 'n', 's', 'one', 'kiss', '(', 'with', 'dua', 'lipa', ')', 'havana', '(', 'feat', '.', 'young', 'thug', ')', 'i', 'like', 'it', 'new', 'rules', 'there', \"'s\", 'nothing', 'holdin', \"'\", 'me', 'back', 'no', 'tears', 'left', 'to', 'cry', 'idgaf', 'in', 'my', 'blood', 'wolves', 'dusk', 'till', 'dawn', '-', 'radio', 'edit', 'attention', 'electricity', '(', 'with', 'dua', 'lipa', ')', 'love', 'on', 'the', 'brain', 'let', 'me', 'go', '(', 'with', 'alesso', ',', 'florida', 'georgia', 'line', '&', 'watt', ')', 'silence', 'sorry', 'not', 'sorry', 'shallow', '-', 'radio', 'edit', 'these', 'days', 'what', 'lovers', 'do', '(', 'feat', '.', 'sza', ')', 'finesse', '-', 'remix', ';', 'feat', '.', 'cardi', 'b', 'perfect', 'duet', '(', 'ed', 'sheeran', '&', 'beyoncé', ')', 'bad', 'at', 'love', 'him', '&', 'i', '(', 'with', 'halsey', ')', 'friends', '(', 'with', 'bloodpop®', ')', 'wild', 'thoughts', '(', 'feat', '.', 'rihanna', '&', 'bryson', 'tiller', ')', 'my', 'my', 'my', '!', 'capital', 'letters', 'sick', 'boy', 'tequila', 'look', 'what', 'you', 'made', 'me', 'do', 'youth', '(', 'feat', '.', 'khalid', ')', 'bad', 'liar', 'anywhere', 'say', 'something', 'chun-li', 'sign', 'of', 'the', 'times', 'familiar', 'let', 'me', 'supernova', 'nervous', 'first', 'time', 'end', 'game', 'mi', 'gente', '(', 'feat', '.', 'beyoncé', ')', 'lemon', 'for', 'you', '(', 'with', 'rita', 'ora', ')', 'want', 'to', 'what', 'i', 'need', '(', 'feat', '.', 'kehlani', ')', 'wait', 'what', 'about', 'us', 'kissing', 'strangers', '2u', '(', 'feat', '.', 'justin', 'bieber', ')', 'walk', 'on', 'water', '(', 'feat', '.', 'beyoncé', ')', 'this', 'town', 'girls', '(', 'feat', '.', 'cardi', 'b', ',', 'bebe', 'rexha', '&', 'charli', 'xcx', ')', 'move', 'to', 'miami', 'miss', 'you', '(', 'with', 'major', 'lazer', '&', 'tory', 'lanez', ')', 'filthy', 'never', 'be', 'the', 'same', '-', 'radio', 'edit', 'ferrari', 'supplies', 'boom', 'boom', '...', 'ready', 'for', 'it', '?', '-', 'bloodpop®', 'remix', 'drip', '(', 'feat', '.', 'migos', ')', 'tell', 'me', 'you', 'love', 'me', '-', 'notd', 'remix', 'memories', 'lose', 'you', 'to', 'love', 'me', 'someone', 'you', 'loved', 'señorita', 'how', 'do', 'you', 'sleep', '?', 'south', 'of', 'the', 'border', '(', 'feat', '.', 'camila', 'cabello', '&', 'cardi', 'b', ')', 'trampoline', '(', 'with', 'zayn', ')', 'happier', 'truth', 'hurts', 'good', 'as', 'hell', '(', 'feat', '.', 'ariana', 'grande', ')', '-', 'remix', 'higher', 'love', 'only', 'human', 'beautiful', 'people', '(', 'feat', '.', 'khalid', ')', 'sucker', 'do', \"n't\", 'call', 'me', 'up', 'i', 'do', \"n't\", 'care', '(', 'with', 'justin', 'bieber', ')', 'talk', '(', 'feat', '.', 'disclosure', ')', 'giant', '(', 'with', \"rag'n'bone\", 'man', ')', 'takeaway', 'all', 'around', 'the', 'world', '(', 'la', 'la', 'la', ')', 'girls', 'like', 'you', '(', 'feat', '.', 'cardi', 'b', ')', 'call', 'you', 'mine', 'no', 'guidance', '(', 'feat', '.', 'drake', ')', 'antisocial', '(', 'with', 'travis', 'scott', ')', 'taki', 'taki', '(', 'feat', '.', 'selena', 'gomez', ',', 'ozuna', '&', 'cardi', 'b', ')', 'con', 'calma', '-', 'remix', 'find', 'u', 'again', '(', 'feat', '.', 'camila', 'cabello', ')', 'cross', 'me', '(', 'feat', '.', 'chance', 'the', 'rapper', '&', 'pnb', 'rock', ')', 'no', 'brainer', '(', 'feat', '.', 'justin', 'bieber', ',', 'chance', 'the', 'rapper', '&', 'quavo', ')', 'nothing', 'breaks', 'like', 'a', 'heart', '(', 'feat', '.', 'miley', 'cyrus', ')', 'kills', 'you', 'slowly']\n\n\n\ndef score_words(words):\n  positive = []\n  negative = []\n  for i in words:\n    scores = sia.polarity_scores(i)\n    if scores[\"compound\"] &gt; 0:\n      positive.append(i)\n    elif scores[\"compound\"] &lt; 0:\n      negative.append(i)\n  return positive, negative\n\npositive, negative = score_words(words)\nprint(negative)\nprint(positive)\n\n['bad', 'broken', 'dirty', 'misery', 'blah', 'blah', 'blah', 'broken', 'bitch', 'hard', 'lost', 'hell', 'trouble', 'fire', 'kill', 'stop', 'stop', 'worry', 'attack', 'die', 'sadness', 'lose', 'bitch', 'lose', 'hard', 'killed', 'heartbreaker', 'fire', 'hunger', 'crazy', 'foolish', 'freak', 'rude', 'shake', 'problem', 'dirty', 'killed', 'forget', 'drunk', 'weapons', 'bad', 'sorry', 'forget', 'blame', 'jealous', 'hurt', 'lose', 'broken', 'hard', 'steal', 'fire', 'dangerous', 'stop', 'lose', 'dangerous', 'starving', 'fire', 'no', 'kill', 'lose', 'fool', 'cruel', 'wtf', 'hurts', 'problems', 'lies', 'crying', 'tired', 'cut', 'bummer', 'low', 'no', 'no', 'tears', 'cry', 'sorry', 'sorry', 'bad', 'sick', 'bad', 'liar', 'nervous', 'miss', 'lose', 'hurts', 'hell', 'sucker', 'no', 'no', 'kills']\n['love', 'romance', 'dynamite', 'like', 'love', 'want', 'like', 'dream', 'kiss', 'ha', 'sexy', 'original', 'original', 'love', 'like', 'like', 'love', 'top', 'best', 'party', 'good', 'yeah', 'sexy', 'glory', 'please', 'love', 'cheers', 'super', 'heaven', 'paradise', 'beautiful', 'love', 'like', 'love', 'stronger', 'good', 'glad', 'beauty', 'love', 'party', 'kiss', 'fame', 'lucky', 'clarity', 'best', 'kiss', 'sweet', 'brave', 'beautiful', 'love', 'love', 'love', 'play', 'love', 'party', 'like', 'love', 'applause', 'beautiful', 'like', 'happy', 'treasure', 'love', 'adore', 'love', 'good', 'perfect', 'love', 'yeah', 'party', 'grey', 'love', 'love', 'love', 'like', 'grey', 'grey', 'grey', 'want', 'want', 'love', 'love', 'worth', 'free', 'dear', 'care', 'love', 'like', 'love', 'want', 'easy', 'love', 'feeling', 'like', 'love', 'like', 'alive', 'heroes', 'perfect', 'love', 'celebrate', 'original', 'lovers', 'spark', 'lover', 'love', 'love', 'feeling', 'original', 'ty', 'beautiful', 'like', 'love', 'confident', 'good', 'like', 'original', 'kindness', 'cool', 'perfect', 'like', 'thrills', 'love', 'lover', 'desire', 'wish', 'peculiar', 'original', 'true', 'like', 'champagne', 'beautiful', 'treat', 'better', 'like', 'love', 'mercy', 'like', 'chance', 'praying', 'greatest', 'love', 'rich', 'love', 'love', 'feeling', 'ok', 'kissing', 'love', 'rich', 'free', 'kissing', 'kiss', 'like', 'love', 'lovers', 'perfect', 'love', 'friends', 'want', 'kissing', 'ready', 'love', 'love', 'loved', 'happier', 'truth', 'good', 'love', 'beautiful', 'care', 'like', 'chance', 'chance', 'like']\n\n\n\nmost_positive = nltk.FreqDist(positive).most_common(15)\nplt.barh([x[0] for x in most_positive],[x[1] for x in most_positive], color = \"lightsteelblue\")\nplt.show()\n\n\n\n\n\nmost_negative = nltk.FreqDist(negative).most_common(15)\nplt.barh([x[0] for x in most_negative],[x[1] for x in most_negative], color = \"maroon\")\nplt.show()\n\n\n\n\nThe above data analysis can determine what kind of titles are frequently used for popular songs. Negative words appear far less often in the “title” column, as demonstrated by the maroon figure. Love appears around 40 times in the titles of the most popular tracks, which indicates that it is a commonly used word in song titles. While I cannot deduce whether love is used in a positive or negative context within the title, the use of the word alone is enough to hint towards either a romantic or breakup type of song.\nHere I will include more in depth analysis about the words in the song titles, and further discuss the data visualizations listed below. I hope to better understand what components go into a popular song, and to determine if there are any correlations between variables.\n\nData Visualizations to Include\n\nMore scatter plots comparing popularity to other variables\n\npop vs. dnce\npop vs. bpm\npop vs. val\n\nKaty Perry’s (or a specific artist) song trends\n\nyear vs. pop\ntrends in valence\n\nTop songs for 2010: trends in valence/dnce\nDance pop songs: dnce vs. bpm\n\npossibly comparing these with another genre, or comparing 2 smaller genres to see if there is a difference in overall trends?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]